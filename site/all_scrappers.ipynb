{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are all the different scrappers each one for a different website :\n",
    "\n",
    "-Nike\n",
    "\n",
    "-Franprix \n",
    "\n",
    "-Auchan \n",
    "\n",
    "-Darty\n",
    "\n",
    "-Fnac \n",
    "\n",
    "-Adidas \n",
    "\n",
    "-Dell \n",
    "\n",
    "-Back Market  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage\n",
      "Data successfully written to decathlon_promotions.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data: Status code 403\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "\n",
    "# # URL of the webpage to scrape\n",
    "# # url = 'https://www.decathlon.fr/deals-page/_/N-7d8dux'\n",
    "# # url = 'https://www.decathlon.fr'\n",
    "\n",
    "# # Send a GET request to the URL\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     # Parse the content with BeautifulSoup\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     # Find all product divs\n",
    "#     products = soup.find_all('div', class_='dkt-product__infos__link')\n",
    "\n",
    "#     # Open a CSV file to write the data\n",
    "#     with open('decathlon_promotions.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['Product Name', 'Original Price (€)', 'Promotional Price (€)', 'Discount Percentage'])\n",
    "\n",
    "#         for product in products:\n",
    "#             product_name = product.text.strip()\n",
    "#             price_info = product.find_next_sibling(\"div\", class_=\"dkt-product js-product-slider-init\")\n",
    "            \n",
    "#             if price_info:\n",
    "#                 original_price = price_info.find('span', class_='dkt-price__previous-price').text.strip().replace('€', '')\n",
    "#                 promotional_price = price_info.find('div', class_='dkt-price__cartridge').text.strip().replace('€', '')\n",
    "#                 discount_percentage = price_info.find('span', class_='dkt-price__reduction').text.strip()\n",
    "\n",
    "#                 writer.writerow([product_name, original_price, promotional_price, discount_percentage])\n",
    "\n",
    "#     print('Data successfully written to decathlon_promotions.csv')\n",
    "# else:\n",
    "#     print(f\"Failed to retrieve data: Status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nike scrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = 'https://www.nike.com/fr/w/promotions-3yaep'\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print('Failed to retrieve the webpage')\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all product cards\n",
    "product_cards = soup.find_all('div', class_='product-card__body')\n",
    "\n",
    "# Create a CSV file\n",
    "with open('nike_promotions.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Product Name', 'Original Price (€)', 'Current Price (€)', 'Promotion Percentage'])\n",
    "\n",
    "    for card in product_cards:\n",
    "        product_name = card.find('div', class_='product-card__title').text.strip()\n",
    "        current_price = card.find('div', class_='product-price is--current-price css-1ydfahe').text.strip().replace('€', '').replace(',', '.')\n",
    "        original_price = card.find('div', class_='product-price fr__styling is--striked-out css-0').text.strip().replace('€', '').replace(',', '.')\n",
    "        \n",
    "        # Calculate promotion percentage\n",
    "        try:\n",
    "            current_price = float(current_price)\n",
    "            original_price = float(original_price)\n",
    "            promotion_percentage = ((original_price - current_price) / original_price) * 100\n",
    "        except ValueError:\n",
    "            # If there's an issue with parsing prices, skip this product\n",
    "            continue\n",
    "\n",
    "        writer.writerow([product_name, original_price, current_price, f'{promotion_percentage:.2f}%'])\n",
    "\n",
    "print('Data successfully written to nike_promotions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "franprix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up WebDriver\n",
    "# driver_path = 'path_to_your_chromedriver.exe'  # Replace with your ChromeDriver path\n",
    "driver_path = 'C:/Users/hugod/AppData/Local/Programs/Python/Python310/chromedriver.exe'\n",
    "\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://www.franprix.fr/courses/promotions'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the dynamic content to load\n",
    "time.sleep(8)\n",
    "\n",
    "# Find elements containing the promotions\n",
    "promotions = driver.find_elements(By.CSS_SELECTOR, 'div.product-item__promo__regular span')\n",
    "\n",
    "# Extract text from each element\n",
    "promo_texts = [promo.text for promo in promotions]\n",
    "\n",
    "# Process text to extract percentage values\n",
    "percentages = []\n",
    "for text in promo_texts:\n",
    "    if '%' in text:\n",
    "        # Extract and clean the percentage value\n",
    "        percentage = text.split('%')[0].strip()\n",
    "        percentages.append(percentage)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(percentages, columns=['Discount Percentage'])\n",
    "df.to_csv('promotions_franprix1.csv', index=False)\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Output the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "darty scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.darty.com/nav/operation/bons-plans-gros-electromenager#dartyclic=PO-bons-plan_BLGF_deco-nos-prom_0_gros-elec'\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all article elements that have a class 'product'\n",
    "    products = soup.find_all('article', class_='product')\n",
    "\n",
    "    # Open a CSV file to write the data\n",
    "    with open('darty_promo.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header of the CSV file\n",
    "        writer.writerow(['Product Name', 'Original Price (€)', 'Current Price (€)', 'Promotion Percentage'])\n",
    "\n",
    "        # Loop over all product articles found\n",
    "        for product in products:\n",
    "            # Extract the product name\n",
    "            name = product.find('a', class_='name').text.strip()\n",
    "\n",
    "            # Extract the original and current prices\n",
    "            price_promo = product.find('div', class_='price price_promo').text.strip().replace('€', '').replace(',', '.').replace(u'\\xa0', u' ')\n",
    "            striped_price = product.find('div', class_='striped_price').text.strip().replace('€', '').replace(',', '.').replace(u'\\xa0', u' ')\n",
    "            \n",
    "            # Calculate the promotion percentage\n",
    "            original_price = float(striped_price)\n",
    "            current_price = float(price_promo)\n",
    "            promotion_percentage = ((original_price - current_price) / original_price) * 100\n",
    "\n",
    "            # Write the product data to the CSV file\n",
    "            writer.writerow([name, original_price, current_price, f'{promotion_percentage:.2f}%'])\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fnac scrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ////////////////////////\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set up the WebDriver with Service\n",
    "driver_path = 'C:/Users/hugod/AppData/Local/Programs/Python/Python310/chromedriver.exe'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# # ////////////////////\n",
    "# # Set up the WebDriver (example with Chrome)\n",
    "# driver_path = 'C:/Users/hugod/AppData/Local/Programs/Python/Python310/chromedriver.exe'\n",
    "# driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "# Navigate to the website\n",
    "url = 'https://www.fnac.com/Toutes-nos-Offres-TV-Video-Home-cinema/shi74185/w-4#bl=MMtvh'\n",
    "driver.get(url)\n",
    "\n",
    "# Give some time for the page to load\n",
    "time.sleep(14)\n",
    "\n",
    "# Find elements by class name and extract text\n",
    "prices_red = driver.find_elements(By.CLASS_NAME, 'price.red')\n",
    "standard_prices = driver.find_elements(By.CLASS_NAME, 'Prix.standard')\n",
    "discounts = driver.find_elements(By.CSS_SELECTOR, 'span.stimuliOPC-flyer-price span')\n",
    "\n",
    "# Prepare data for CSV\n",
    "data = []\n",
    "for price, standard_price, discount in zip(prices_red, standard_prices, discounts):\n",
    "    data.append([price.text, standard_price.text, discount.text])\n",
    "\n",
    "# Write data to a CSV file\n",
    "with open('product_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Price (Red)', 'Standard Price', 'Discount'])\n",
    "    writer.writerows(data)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auchan scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Configure Selenium to use Chrome in headless mode\n",
    "options = Options()\n",
    "options.headless = True\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "chromedriver_path = 'C:/Users/hugod/AppData/Local/Programs/Python/Python310/chromedriver.exe'\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# URL to scrape\n",
    "# url = 'https://example.com/promotions'  # Replace with the actual URL\n",
    "url= 'https://www.auchan.fr/boutique/promos'\n",
    "\n",
    "# Selenium navigates to the page\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "driver.implicitly_wait(10)  # Adjust time as needed\n",
    "\n",
    "# Get the page source from Selenium\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Now use BeautifulSoup to parse the page source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find elements containing the product information\n",
    "product_elements = soup.find_all('p', class_='product-thumbnail__description')\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = 'promotion_auchan1.csv'  # Update this path\n",
    "\n",
    "# Create a CSV file to store the data\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Brand', 'Description'])\n",
    "\n",
    "    for product in product_elements:\n",
    "        brand = product.find('strong', itemprop='brand').text if product.find('strong', itemprop='brand') else 'N/A'\n",
    "        description = product.get_text(separator=' ', strip=True)\n",
    "        writer.writerow([brand, description])\n",
    "\n",
    "print(f'Data written to {csv_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we gonna aggregate the data scrappe from the previous site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a dataframe that will hold the average of percentage of reduction for each brand\n",
    "import pandas as pd\n",
    "\n",
    "df_average = pd.DataFrame(columns=['website_name','Discount Percentage average','website'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we take all the output from the csv and create a unique csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_19932\\3464572383.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_average = df_average.append({'website_name':'darty','Discount Percentage average':mean_darty,'website':'https://www.darty.com/nav/operation/bons-plans-gros-electromenager#dartyclic=PO-bons-plan_BLGF_deco-nos-prom_0_gros-elec'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_darty = pd.read_csv('darty_promo.csv')\n",
    "df_darty[\"Promotion Percentage\"]= df_darty[\"Promotion Percentage\"].str.replace('%', '')\n",
    "df_darty[\"Promotion Percentage\"]= df_darty[\"Promotion Percentage\"].astype(float)\n",
    "mean_darty = df_darty[\"Promotion Percentage\"].mean()\n",
    "df_average = df_average.append({'website_name':'darty','Discount Percentage average':mean_darty,'website':'https://www.darty.com/nav/operation/bons-plans-gros-electromenager#dartyclic=PO-bons-plan_BLGF_deco-nos-prom_0_gros-elec'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_19932\\2815849369.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_average = df_average.append({'website_name':'franprix','Discount Percentage average':mean_franprix,'website':'https://www.franprix.fr/courses/promotions'}, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discount Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Discount Percentage\n",
       "0                   25\n",
       "1                   30\n",
       "2                   30\n",
       "3                   50\n",
       "4                   50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_franprix = pd.read_csv('promotions_franprix1.csv')\n",
    "df_franprix[\"Discount Percentage\"]= df_franprix[\"Discount Percentage\"].abs()\n",
    "mean_franprix = df_franprix[\"Discount Percentage\"].mean()\n",
    "df_average = df_average.append({'website_name':'franprix','Discount Percentage average':mean_franprix,'website':'https://www.franprix.fr/courses/promotions'}, ignore_index=True)\n",
    "df_franprix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_19932\\2640905431.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_average = df_average.append({'website_name':'dell','Discount Percentage average':mean_dell,'website':'https://www.dell.com/fr-fr/shop/offres/ordinateurs-portables'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_dell = pd.read_csv('dell.csv')\n",
    "# df_dell.head()\n",
    "# df_dell[\"Discount Percentage\"]= len(df_dell[\"Discount Percentage\"].isnumeric())\n",
    "# len(df_dell[\"Discount Percentage average\"].type())\n",
    "# keep only numeric values of the column Discount Percentage\n",
    "# df_dell[\"Discount Percentage average\"]= df_dell[df_dell[\"Discount Percentage average\"].str.isnumeric() == True]\n",
    "# df_dell[\"Discount Percentage average\"]= df_dell[\"Discount Percentage average\"].astype(float)\n",
    "# df_dell.describe()\n",
    "df_dell.dropna(subset=['Discount Percentage average'], inplace=True)\n",
    "df_dell.describe()\n",
    "df_dell[\"Discount Percentage average\"]= df_dell[\"Discount Percentage average\"].str.replace('%', '')\n",
    "#make the average of the column Discount Percentage average\n",
    "df_dell[\"Discount Percentage average\"]= df_dell[\"Discount Percentage average\"].astype(float)\n",
    "\n",
    "df_dell[\"Discount Percentage average\"].mean()\n",
    "mean_dell = df_dell[\"Discount Percentage average\"].mean()\n",
    "df_average = df_average.append({'website_name':'dell','Discount Percentage average':mean_dell,'website':'https://www.dell.com/fr-fr/shop/offres/ordinateurs-portables'}, ignore_index=True)\n",
    "\n",
    "# df_dell.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_19932\\48700532.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_average = df_average.append({'website_name':'auchan','Discount Percentage average':mean_auchan,'website':'https://www.auchan.fr/boutique/promos'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_auchan = pd.read_csv('promotion_auchan1.csv')\n",
    "\n",
    "# Find rows with a percentage reduction in 'Description'\n",
    "df_auchan[\"Contains_Percentage\"] = df_auchan[\"Description\"].str.contains('%')\n",
    "\n",
    "# Keep only the rows with a percentage of reduction\n",
    "df_auchan = df_auchan[df_auchan[\"Contains_Percentage\"] == True]\n",
    "\n",
    "# Extract the numeric values (including decimals) of the column 'Description' which are percentages\n",
    "df_auchan[\"Percentage\"] = df_auchan[\"Description\"].str.extract('(\\d+\\.\\d+|\\d+)').astype(float)\n",
    "\n",
    "# Make the average of the extracted percentages\n",
    "mean_auchan = df_auchan[\"Percentage\"].mean()\n",
    "\n",
    "mean_auchan\n",
    "\n",
    "df_average = df_average.append({'website_name':'auchan','Discount Percentage average':mean_auchan,'website':'https://www.auchan.fr/boutique/promos'}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.34375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugod\\AppData\\Local\\Temp\\ipykernel_19932\\792152401.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_average = df_average.append({'website_name':'nike','Discount Percentage average':mean_nike,'website':'https://www.nike.com/fr/w/promotions-3yaep'}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_nike = pd.read_csv('nike_promotions.csv')\n",
    "df_nike[\"Promotion Percentage\"]= df_nike[\"Promotion Percentage\"].str.replace('%', '')\n",
    "df_nike[\"Promotion Percentage\"]= df_nike[\"Promotion Percentage\"].astype(float)\n",
    "mean_nike = df_nike[\"Promotion Percentage\"].mean()\n",
    "df_average = df_average.append({'website_name':'nike','Discount Percentage average':mean_nike,'website':'https://www.nike.com/fr/w/promotions-3yaep'}, ignore_index=True)\n",
    "\n",
    "print(mean_nike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no we will save the df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average.to_csv('average.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
